# app/streamlit_app.py

import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path

import streamlit.components.v1 as components

# ì‹œê°í™” / ML ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

import matplotlib.pyplot as plt

import networkx as nx
from pyvis.network import Network

# ì„ íƒì : UMAP, SciPy (ì—†ìœ¼ë©´ graceful fallback)
try:
    import umap
    HAS_UMAP = True
except ImportError:
    HAS_UMAP = False

try:
    from scipy.cluster.hierarchy import linkage, dendrogram
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


# --------------------------------------------------
# ê¸°ë³¸ ì„¤ì • ë° ë°ì´í„° ë¡œë”©
# --------------------------------------------------

ROOT = Path(__file__).resolve().parents[1]
OUT = ROOT / "outputs"

st.set_page_config(page_title="ICPSR Dataset Citation Detector", layout="wide")
st.title("ICPSR Dataset Citation Detector â€” Dashboard")

# íŒŒì¼ ê²½ë¡œ (íŒŒì´í”„ë¼ì¸ì—ì„œ ìƒì„±ë˜ëŠ” ì´ë¦„ê³¼ ì¼ì¹˜ì‹œí‚´)
articles_fp = OUT / "icpsr_articles_detected.csv"      # ë…¼ë¬¸ ë‹¨ìœ„ ê²°ê³¼
datasets_fp = OUT / "icpsr_datasets_detected.csv"      # ë°ì´í„°ì…‹ ë‹¨ìœ„ summary
clusters_fp = OUT / "clusters.csv"                     # ì„ íƒì 


# í•„ìˆ˜ íŒŒì¼ ì²´í¬
if not articles_fp.exists():
    st.warning(
        "No article-level outputs found.\n\n"
        "Run the pipeline first, e.g.:\n"
        "`python scripts/pipeline.py`"
    )
    st.stop()


@st.cache_data
def load_data():
    """CSVë“¤ì„ í•œ ë²ˆë§Œ ì½ì–´ì„œ ìºì‹œ."""
    # ë…¼ë¬¸ ë‹¨ìœ„ ê²°ê³¼
    arts = pd.read_csv(articles_fp)

    # icpsr_ids ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ íŒŒì‹± ì‹œë„ (ì—†ìœ¼ë©´ ê±´ë„ˆëœ€)
    if "icpsr_ids" in arts.columns and arts["icpsr_ids"].dtype == object:
        try:
            arts["icpsr_ids"] = arts["icpsr_ids"].apply(
                lambda s: eval(s) if isinstance(s, str) and isinstance(s, str) and s.startswith("[") else s
            )
        except Exception:
            # ì´ìƒí•˜ë©´ ê·¸ëƒ¥ ì›ë³¸ ìœ ì§€
            pass

    # ë°ì´í„°ì…‹ summary / í´ëŸ¬ìŠ¤í„° (ì—†ìœ¼ë©´ ë¹ˆ DF)
    dsets = pd.read_csv(datasets_fp) if datasets_fp.exists() else pd.DataFrame()
    clus = pd.read_csv(clusters_fp) if clusters_fp.exists() else pd.DataFrame()

    return arts, dsets, clus


articles, datasets, clusters = load_data()


# --------------------------------------------------
# í—¬í¼ í•¨ìˆ˜ë“¤
# --------------------------------------------------

def filter_articles(df: pd.DataFrame, q: str, only_hits: bool,
                    year_min: int, year_max: int) -> pd.DataFrame:
    """ê²€ìƒ‰ì–´ / has_icpsr / ì—°ë„ ë²”ìœ„ë¡œ articlesë¥¼ í•„í„°ë§."""
    f = df.copy()

    # ICPSR ê²€ì¶œëœ ë…¼ë¬¸ë§Œ ë³´ê¸°
    if only_hits and "has_icpsr" in f.columns:
        f = f[f["has_icpsr"] == True]

    # í…ìŠ¤íŠ¸ ê²€ìƒ‰ (title / doi / authors / journal)
    if q:
        ql = q.lower()
        cols = ["title", "doi", "authors", "journal"]
        mask = False
        for c in cols:
            if c in f.columns:
                m = f[c].fillna("").astype(str).str.lower().str.contains(ql)
                mask = m if isinstance(mask, bool) else (mask | m)
        if not isinstance(mask, bool):
            f = f[mask]

    # ì—°ë„ í•„í„° (year ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ)
    if "year" in f.columns:
        try:
            years = pd.to_numeric(f["year"], errors="coerce")
            f = f[(years >= year_min) & (years <= year_max)]
        except Exception:
            pass

    return f


def get_dataset_feature_matrix(dsets: pd.DataFrame):
    """
    UMAP / t-SNE / PCA / dendrogramìš© feature matrix ìƒì„±.
    ì£¼ë¡œ ìˆ«ìí˜• ì»¬ëŸ¼ë“¤(n_articles, max_detection_score, mean_detection_score)ì„ ì‚¬ìš©.
    """
    if dsets.empty:
        return None, None

    df = dsets.copy()

    # í›„ë³´ numeric ì»¬ëŸ¼
    candidate_cols = [
        "n_articles",
        "max_detection_score",
        "mean_detection_score",
    ]
    num_cols = [c for c in candidate_cols if c in df.columns]

    # ì—†ìœ¼ë©´ ë‹¤ë¥¸ numeric ì»¬ëŸ¼ ì°¾ê¸°
    if not num_cols:
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    if not num_cols:
        return None, None

    # NaN ì œê±°
    df_num = df[num_cols].copy()
    df_num = df_num.replace([np.inf, -np.inf], np.nan).dropna()
    df = df.loc[df_num.index]

    if df_num.empty:
        return None, None

    X = df_num.values.astype(float)

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return df, X_scaled


def build_bipartite_graph(articles_df: pd.DataFrame,
                          max_articles: int = 200,
                          max_datasets: int = 200):
    """
    ë…¼ë¬¸-ë°ì´í„°ì…‹ ì´ë¶„ ê·¸ë˜í”„ ìƒì„±.
    ë…¸ë“œ: article, dataset(icpsr_study_number)
    ì—£ì§€: article -> dataset
    """
    if "icpsr_study_number" not in articles_df.columns:
        return None

    df = articles_df.copy()
    df = df[df["icpsr_study_number"].notna()]

    if df.empty:
        return None

    # ì œí•œ ê±¸ê¸° (í° ê·¸ë˜í”„ ë°©ì§€)
    df = df.iloc[:max_articles]

    G = nx.Graph()

    # ë°ì´í„°ì…‹ ë…¸ë“œ ì œí•œ
    dataset_values = df["icpsr_study_number"].unique()[:max_datasets]
    allowed_datasets = set(dataset_values)

    for idx, row in df.iterrows():
        art_id = f"ART:{idx}"
        title = str(row.get("title", ""))[:80]
        study = row["icpsr_study_number"]

        if study not in allowed_datasets:
            continue

        ds_id = f"DS:{study}"

        # article node
        G.add_node(
            art_id,
            label=f"Article\n{title}",
            bipartite="article",
        )

        # dataset node
        G.add_node(
            ds_id,
            label=f"ICPSR {study}",
            bipartite="dataset",
        )

        # edge
        G.add_edge(art_id, ds_id)

    if G.number_of_nodes() == 0:
        return None

    return G


def render_pyvis_graph(G: nx.Graph, height: str = "600px"):
    """
    PyVisë¡œ bipartite ê·¸ë˜í”„ë¥¼ ë Œë”ë§í•˜ê³  Streamlitì— embed.
    """
    net = Network(height=height, width="100%", notebook=False, bgcolor="#ffffff", font_color="black")
    net.barnes_hut()

    # PyVis graphë¡œ ë³€í™˜
    for node, data in G.nodes(data=True):
        label = data.get("label", node)
        group = data.get("bipartite", "other")
        net.add_node(node, label=label, group=group)

    for u, v in G.edges():
        net.add_edge(u, v)

    # HTML ìƒì„±
    html = net.generate_html(notebook=False)
    components.html(html, height=600, scrolling=True)


# --------------------------------------------------
# ì‚¬ì´ë“œë°” / í•„í„° UI
# --------------------------------------------------

with st.expander("Search / Filter", expanded=True):
    q = st.text_input("Filter articles by title / DOI / author / journal", "")
    only_hits = st.checkbox("Show only articles with ICPSR mentions", value=True)
    year_min, year_max = st.slider(
        "Year range (if available in data)",
        1900,
        2030,
        (1900, 2030),
    )


# --------------------------------------------------
# Articles í…Œì´ë¸”
# --------------------------------------------------

st.subheader("Articles")

filtered_articles = filter_articles(articles, q, only_hits, year_min, year_max)
st.dataframe(
    filtered_articles,
    use_container_width=True,
    height=320,
    hide_index=True,
)

# ìƒì„¸ ì •ë³´ ì„ íƒ (ì›í•˜ë©´)
if not filtered_articles.empty:
    st.markdown("### Article details")
    idx = st.number_input(
        "Select row index",
        min_value=0,
        max_value=len(filtered_articles) - 1,
        value=0,
        step=1,
    )
    row = filtered_articles.iloc[int(idx)]
    st.markdown(f"**Title:** {row.get('title', '')}")
    st.markdown(f"**DOI:** {row.get('doi', '')}")
    st.markdown(f"**ICPSR study number:** {row.get('icpsr_study_number', '')}")
    st.markdown(f"**Detection score:** {row.get('detection_score', '')}")
    st.markdown(f"**Signal type:** {row.get('signal_type', '')}")
    if row.get("snippet"):
        st.code(str(row["snippet"]), language="text")


# --------------------------------------------------
# Datasets & Clusters
# --------------------------------------------------

st.subheader("ICPSR Datasets & Clusters")

if datasets.empty:
    st.info(
        "No dataset-level summary found.\n\n"
        "If you already have article-level results, you can build the "
        "dataset summary by running:\n"
        "`python scripts/dataset_summary_only.py`"
    )
else:
    # ---- ICPSR ë§í¬ ìƒì„± ----
    if "icpsr_study_number" in datasets.columns:
        datasets = datasets.copy()
        datasets["ICPSR Link"] = datasets["icpsr_study_number"].apply(
            lambda x: f"https://www.icpsr.umich.edu/web/ICPSR/studies/{int(x)}"
            if pd.notna(x) else ""
        )

    st.dataframe(
        datasets,
        use_container_width=True,
        height=320,
        hide_index=True,
    )

    st.markdown(
        """
        ğŸ”— **Click the links to view each dataset on ICPSR.org**

        *(Links appear in the â€œICPSR Linkâ€ column above.)*
        """
    )

    # cluster ì»¬ëŸ¼ ìˆì„ ë•Œë§Œ í´ëŸ¬ìŠ¤í„° í•„í„°ë§ ì œê³µ
    if "cluster" in datasets.columns:
        st.markdown("### View by cluster")
        cluster_ids = sorted(datasets["cluster"].dropna().unique())
        sel = st.multiselect(
            "Select clusters to view",
            cluster_ids,
            default=cluster_ids[: min(5, len(cluster_ids))],
        )

        df_cluster_sel = datasets[datasets["cluster"].isin(sel)]

        st.dataframe(
            df_cluster_sel,
            use_container_width=True,
            height=300,
            hide_index=True,
        )

        # ì„ íƒí•œ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ì…‹ ë§í¬
        if not df_cluster_sel.empty:
            st.markdown("### Selected cluster datasets â€” ICPSR Links")
            for _, r in df_cluster_sel.iterrows():
                st.markdown(
                    f"- [{r['icpsr_study_number']} â€” {r.get('title','(no title)')}]"
                    f"({r.get('ICPSR Link','')})"
                )


# --------------------------------------------------
# Cluster Visualization (UMAP / t-SNE / PCA / Dendrogram)
# --------------------------------------------------

if not datasets.empty:
    st.markdown("## Cluster Visualization")

    df_feat, X = get_dataset_feature_matrix(datasets)

    if X is None or df_feat is None:
        st.info("Not enough numeric features to build visualizations.")
    else:
        viz_method = st.radio(
            "Select embedding method",
            ["t-SNE", "UMAP (if available)", "PCA (fallback)"],
            index=0,
        )

        # 2D embedding ê³„ì‚°
        embed_df = None
        if viz_method == "UMAP (if available)":
            if HAS_UMAP:
                reducer = umap.UMAP(n_components=2, random_state=42)
                emb = reducer.fit_transform(X)
                embed_df = pd.DataFrame(emb, columns=["x", "y"], index=df_feat.index)
            else:
                st.warning("UMAP is not installed. Falling back to t-SNE.")
                tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, max(5, X.shape[0] - 1)))
                emb = tsne.fit_transform(X)
                embed_df = pd.DataFrame(emb, columns=["x", "y"], index=df_feat.index)

        elif viz_method == "t-SNE":
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, max(5, X.shape[0] - 1)))
            emb = tsne.fit_transform(X)
            embed_df = pd.DataFrame(emb, columns=["x", "y"], index=df_feat.index)

        else:  # PCA
            pca = PCA(n_components=2, random_state=42)
            emb = pca.fit_transform(X)
            embed_df = pd.DataFrame(emb, columns=["x", "y"], index=df_feat.index)

        # í”Œë¡¯ìš© DataFrame êµ¬ì„±
        if embed_df is not None:
            plot_df = embed_df.copy()
            plot_df["cluster"] = df_feat["cluster"] if "cluster" in df_feat.columns else -1
            plot_df["icpsr_study_number"] = df_feat.get("icpsr_study_number", "")

            st.markdown("### 2D Embedding of Datasets")

            # Altairë¡œ scatter plot
            import altair as alt

            chart = alt.Chart(plot_df.reset_index(drop=True)).mark_circle(size=60).encode(
                x="x",
                y="y",
                color="cluster:N",
                tooltip=["icpsr_study_number", "cluster"]
            ).properties(
                width="container",
                height=400
            )

            st.altair_chart(chart, use_container_width=True)

        # Dendrogram (ì˜µì…˜)
        st.markdown("### Dendrogram (Hierarchical Clustering)")
        if not HAS_SCIPY:
            st.info("SciPy is not installed. Dendrogram is unavailable in this environment.")
        else:
            try:
                linked = linkage(X, method="average")
                fig, ax = plt.subplots(figsize=(8, 4))
                dendrogram(linked, labels=df_feat.get("icpsr_study_number", "").astype(str).values, leaf_rotation=90, ax=ax)
                ax.set_ylabel("Distance")
                st.pyplot(fig)
            except Exception as e:
                st.warning(f"Dendrogram plotting failed: {e}")


# --------------------------------------------------
# Articleâ€“Dataset Bipartite Graph
# --------------------------------------------------

st.markdown("## Articleâ€“Dataset Bipartite Graph")

with st.expander("Bipartite Graph Settings", expanded=False):
    max_articles = st.slider("Max number of articles", 10, 500, 150, step=10)
    max_datasets = st.slider("Max number of datasets", 10, 500, 150, step=10)

G = build_bipartite_graph(articles, max_articles=max_articles, max_datasets=max_datasets)

if G is None:
    st.info("Not enough articleâ€“dataset links to build a bipartite graph.")
else:
    st.markdown(
        "This graph shows articles (one partition) connected to ICPSR datasets (other partition)."
    )
    render_pyvis_graph(G, height="600px")


st.markdown("---")
st.caption(
    "Tip: Re-run the pipeline if you change detection rules. "
    "Article-level: `python scripts/pipeline.py` Â· "
    "Dataset-level only: `python scripts/dataset_summary_only.py`"
)